[TOC]


# Diffusion Models：
**參考：** https://reurl.cc/d10Qz8 

Diffusion model 的概念就像是 VAE：試圖將原始資料投影到潛在空間上，然後再從潛在空間恢復到原始資料。  
它的主要作法是模擬 Markov chain 中的一系列高斯雜訊分佈來逐步地對原始訊號 (影像、音訊) X_0 添加高斯雜訊，進而生成一個服從高斯分佈的訊號 X_T，接著訓練一個學習模型能夠將 X_T 逐步地還原成 X_0。  
1. 從訊號逐步地添加高斯雜訊 (X_0 ➞ … ➞ X_t-1 ➞ X_t ➞ … ➞ X_T) 的擴散過程 (diffusion process): q(X)
2. 從雜訊逐步地還原成訊號 (X_T ➞ … ➞ X_t ➞ X_t-1 ➞ … ➞ X_0) 的逆擴散過程 (reverse process): p_θ(X)

這邊的 q(X) 是不可訓練的，而 p_θ(X) 是可以訓練的，訓練完成後就可以得到我們想要的學習模型。

## Diffusion process (擴散過程)
擴散過程採用固定的 Markov chain 形式，即當前的任何狀態都與過去狀態完全無關，即可逐步地向圖片添加高斯雜訊，給定初始資料分佈 X_0 ~ q(X)，可以不斷的向分佈中添加高斯雜訊，這個雜訊的變異數為 β_t、均值為 β_t 和當前 t-1 時刻的資料 X_t-1 決定的，這個 β_t 是逐漸增加並且介於 0 和 1 之間，隨著雜訊不斷的添加，最終生成資料 X_T。

## Reverse process (逆擴散過程)
逆擴散過程是想要從高斯雜訊中恢復成原始資料，它仍然是一個 Markov chain，我們可以假設它也是一個高斯分佈，所以模型 p_θ 的目標就是要要學習這個高斯分佈的均值和變異數，進而透過連乘來從 X_T 還原成 X_0。

## Training
在訓練模型的過程中，要對 loss 進行梯度下降，而 loss 經由一系列推導簡化後，最終可以得到 L_simple(θ)：
https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lT0qH1DDX5kTxFpbPcTDWw.png
其中 ε 是高斯雜訊，此訓練目標即希望預測的雜訊和真實的雜訊一致，也就是計算兩個雜訊之間的 l2 loss。

## Sampling
將雜訊的影像丟進模型中，然後持續讓影像變得清晰，就完成 sampling 了，其中用於生成的式子如下：
https://miro.medium.com/v2/resize:fit:640/format:webp/1*UWv5SnqBaWJ29tXIHlblLQ.png  
經由數學推導可以得到均值：
https://miro.medium.com/v2/resize:fit:720/format:webp/1*r9Amb4PXOe8qGcPF32Mvqg.png  
最後就可以得到上一個時刻的生成式了：
https://miro.medium.com/v2/resize:fit:786/format:webp/1*ipYvfBbcrhq28ZqiMcqpiw.png  

**較詳細(很數學)：** https://reurl.cc/kMdnkd
補充
## Reverse Process Decoder and L0
在反向過程中，路徑包含許多在連續條件高斯分佈下的轉換。在反向過程結束時，我們試圖生成由整數像素值組成的圖像。因此，我們必須設計一種方法，以獲取所有像素的每個可能像素值的discrete (log)概率。

這是通過將反向擴散鏈中的最後一個轉換設置為獨立離散解碼器來完成的。為了確定給定圖像 x1 的情況下圖像 x0 的概率，我們首先將數據維度之間的獨立性約束：
https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Zc0PAnWokmnhErlJ.png



# Denoising Diffusion Probabilistic Models (DDPM)
**參考：** https://reurl.cc/Eg7bdk 

而ddpm就是希望透過forward process和reverse process來學習真實圖片的distribution。舉例來說：我們想生成貓的圖片，那就需要學習貓的圖片的distribution，如果學到了，那們我們只需要做reverse process，給定一張高斯分佈的雜訊圖片，就能生成出貓的圖片。

## forward process
給定一張乾淨清晰的圖片，我們不斷添加noise到這張圖片上，直到這整張圖片都是noise，且呈現高斯分佈。  
forward process如同前面所說，就是針對一張乾淨的圖片x0，不斷地加入雜訊，直到整張圖片變成一個高斯分佈的雜訊圖。  
並且我們可以得到結論，在任意的timestamp我們都可以透過x0和epsilon得到那個timestamp的結果。  
https://miro.medium.com/v2/resize:fit:786/format:webp/1*HZLTYbjoZpn4fl0L89tEcw.png

## reverse process
給定一張呈現高斯分佈的noise圖片，不斷地做denoise(去雜訊），逐步讓這張圖片越來越清晰，直到生成一張乾淨清晰的圖片。
https://miro.medium.com/v2/resize:fit:786/format:webp/1*xZvjhZj4VpjC_Zt7uYxrrA.png    
我們輸入一張想要去雜訊的xt以及他對應的timestamp t到noise predicter，這個noise predicter就是我們要訓練的模型（在本篇論文為一個Unet架構），他會輸出預測出來的雜訊，最後我們將xt減這個預測出來的noise就能得到xt-1了。  

我們把reverse process 的過程表達上下面這個式子，p theta代表在theta這個參數的情況下給定xt產生xt-1的機率。那這個機率可以表達成一個normal distribution，而我們要做的就是預測這個normal distribution的mean，也就是u theta，variance的部分在本篇論文是固定的。因此我們的目標就在於求出下方這個式子。
https://miro.medium.com/v2/resize:fit:786/format:webp/1*D8briCglPk2FbTrsqvVqSA.png



# Image-to-Image Translation (I2I translation)
**參考：** https://home.gamer.com.tw/creationDetail.php?sn=5535118

而 Pix2pix 則是把條件式生成的 GAN (Conditional GAN) 延伸到 I2I translation。也就是說，辨別器不只是看生成結果，它還要觀察輸入圖片當作判斷真假的條件。



# Generative Adversarial Networks (GAN)
**參考：** https://reurl.cc/qnd9Zp

一次看一張，判斷是真是假。

https://i.imgur.com/SQpDOGW.png  
以上圖為例，如果你把輸入圖片 (也就是鞋子的線稿) 跟真實的鞋子照片一起輸入給辨別器，辨別器要把它們分類為真；但把線稿與假的鞋子圖片 (由生成器產生) 一起輸入給辨別器時，它要把它們分類為假。相對地，生成器的目標就是讓辨別器認為：線稿與生成圖片之配對是真實的。

這樣的訓練過程，就可以學習輸入 (x) 與輸出 (y) 之間的關係，也就是 I2I translation 的目標。

但這個方法最大的缺陷，就是需要成對的資料 (paired data) 來訓練，也就是說你對每一張輸入圖片，都要給定一張相對應的、期望的輸出結果。這在實務上顯然不一定能輕易做到 (例如：你要蒐集一大堆馬和斑馬的照片，還要保證每一對圖片都符合你設想的條件)。

因此我們更傾向使用非成對的資料 (unpaired data)，也就是我們只要求兩個類別 (domain) 各自有足夠的訓練圖片，但不強求它們符合某種對應關係。
但這樣要怎麼限制生成器、使它依照條件生成呢？CycleGAN 提出了 cycle consistency loss。

簡單來說，它會多訓練一組生成器與辨別器，假設我們原本的目標是把馬轉成斑馬，我們會讓另一個生成器去把第一個生成器產生的斑馬圖片再轉回馬的圖片，讓最後這張馬的圖片跟一開始的輸入愈相似愈好。



























