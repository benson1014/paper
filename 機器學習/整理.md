# Abstract
由於其實用性，將擴散模型應用於影像轉換（Image-to-Image Translation，I2I）近期受到越來越多的關注。過去的方法通常將來源影像的資訊注入至每個去噪步驟以進行反覆優化，但這導致了耗時的執行過程。我們提出了一種高效的方法，為擴散模型配備了一個輕量級轉譯器，稱為Diffusion Model Translator (DMT)，以實現影像轉換。


# I. INTRODUCTION
擴散機率模型（Diffusion Probabilistic Model），亦稱擴散模型，是一種生成模型，其核心包含以下兩個過程：
1. 前向擴散過程：逐漸向數據分佈添加噪聲，直到將其轉換為簡單的潛在分佈（例如，高斯分佈）。
2. 反向過程：從潛在分佈中生成隨機樣本，並利用學習到的網絡逆轉擴散過程，從而生成原始數據分佈中的數據點。  

儘管擴散模型的發展迅速，但針對條件生成（conditional generation）的研究相對較少，而條件生成是許多實際應用（如影像轉換任務）的關鍵需求。影像轉換任務（Image-to-Image Translation, I2I）需要將來源影像的某種風格轉換為目標影像的另一種風格，這與無條件生成不同，需要以來源域樣本作為內容指導。

目前的方法在處理I2I任務時，需要在反向去噪的每一步中注入來源樣本的資訊，導致步驟間依賴性強，學習過程效率低下。

***我們的方法***  
本研究提出了一種更高效的方法，透過為預訓練的DDPM（Denoising Diffusion Probabilistic Model）配備一個輕量級的轉譯器，稱為Diffusion Model Translator (DMT)。此方法的核心特點包括：
1. 理論證明：給定兩個不同影像域中的擴散過程，只需在某一特定的中間時間步驟進行域間分佈的轉換，即可完成I2I任務，且具有適當的參數化。
2. 高效流程：基於上述理論，設計了一個新的高效DDPM管道（見圖1(b)），通過前向擴散處理來源域和目標域的數據至預定的時間步驟，並使用神經網絡完成典型的I2I轉換。(這句看不懂)

***方法優勢***  
1. 訓練獨立性：DMT的訓練過程與DDPM無關，能以高效率執行。
2. 性能提升：DMT可以結合I2I領域的現有技術（如Pix2Pix、TSIT、SPADE及SEAN），進一步提升性能。
3. 時間步驟選擇：我們提出了一種實用策略，能自動根據數據分佈選擇合適的域轉移時間步驟 t。

***實驗驗證***  
為驗證方法的有效性，我們對以下四個I2I任務進行了實驗：
1. 影像風格化（Image Stylization）
2. 影像上色（Image Colorization）
3. 分割圖到影像（Segmentation to Image）
4. 草圖到影像（Sketch to Image）

通過實驗，無論在質量還是效率方面，我們的方法均優於現有基於擴散模型和生成對抗網絡（GAN）的方法。這些結果從定性與定量兩方面，充分展示了我們方法的優越性能。


# II. RELATED WORK
在前向擴散過程中，擴散機率模型（Diffusion Probabilistic Model, DPM）將給定的數據分佈轉換為簡單的潛在分佈，例如高斯分佈。由於其強大的能力，DPM在許多領域中取得了顯著成功，包括語音合成 [16][17]、視頻合成 [18][19]、影像超解析 [20][21]、條件生成 [10][12] 以及影像轉換 [8][9]。

***去噪擴散機率模型(DDPM)*** 假設前向擴散過程具有馬爾可夫鏈（Markov Chain）。對於一個影像數據集，前向擴散過程通過向每張影像 $x_0$ 添加標準高斯噪聲，使其逐漸轉變為完全隨機的噪聲影像。形式化表示如下：
![](https://imgur.com/a/wvohCcp)  

https://imgur.com/a/hB9JTPy  

***快速擴散模型的探索***  
更快的DPM嘗試通過縮短軌跡來提高效率，而非完成整個反向過程，同時保證合成性能與原始DPM相當。一些現有方法通過網格搜索尋找軌跡 [16]，但這僅適用於短的反向過程，因為其時間複雜度隨指數增長。其他方法嘗試通過動態規劃（DP）算法解決最小成本路徑問題 [22][23]。另一些快速採樣方法則利用高階微分方程（DE）求解器 [24][25][26][27][28]。  
基於生成對抗網絡（GAN）的某些方法也考慮了更大的採樣步長。例如，[29] 展示了在條件GAN中使用更大步長學習多模態分佈的過程。

***影像轉換 (I2I)***  
影像轉換旨在將來源域中的輸入影像轉換為目標域中的影像，通常**需要輸入與輸出配對的訓練數據** [7]。為此，條件生成對抗網絡（Conditional Generative Adversarial Network, cGAN）被設計為通過對抗損失，將輸入影像的資訊注入到生成解碼器中 [30][31]。  
基於cGAN的算法在許多I2I任務中表現出了高質量 [13][14][15][32][33][34][35][36][37][38]。然而，由於其訓練的不穩定性和模式崩潰問題，基於cGAN的方法難以生成多樣化的高分辨率影像。  
最近，DPM已被應用於I2I任務。Palette [9] 引入了一種新穎的DPM框架，通過在每個採樣步驟中注入輸入信息來進行優化。一些方法則使用預訓練的影像合成模型進行I2I任務 [12]。儘管這些方法生成的影像質量很高，但生成過程極為耗時。  

***本研究的解決方案***  
我們的研究針對上述問題，提出了一種新的高效DDPM方法，用於I2I任務。該方法**不需要在每個去噪步驟中注入輸入來源信息**，從而大幅減少時間消耗。  
儘管未配對數據在轉換任務中更易獲取，但配對影像轉換（I2I）的優勢（例如較低的數據需求和更高的合成質量）使其成為一個重要的研究焦點。


#  III. METHOD
###  A. Markov Process of Translation Mappings
對於影像轉換任務（I2I），傳統的DDPM方法直接近似真實分佈 𝑞(𝑦0|𝑥0)，其中 𝑥0 和 𝑦0 是 source domain 𝐷𝑥 和 target domain 𝐷𝑦 中的配對數據。而我們的方法構建了一個 translation module 𝑝𝜃(𝑦𝑡|𝑥𝑡)​ ，**用於連接輸入條件與 pre-trained 的 DDPM**。透過此模組，我們可以利用學習到的中間 translation module 來近似 𝑞(𝑦0|𝑥0)。  
具體而言，給定前向擴散過程的噪聲添加計劃：  
https://imgur.com/a/AS5GU5F  

***目標域DDPM的 reverse process***  
https://imgur.com/a/2BA6obL  

***構建 translation mapping***  
https://imgur.com/a/bdQ7xyE

###  B. Translation Mappings of DDPM
令：  
<div style="font-size: 20px; font-family: 'Times New Roman', serif; line-height: 1;">
    <p>&emsp;&emsp;&emsp;
        <em>p</em><sub>&theta;</sub>(<em>y</em><sub>0</sub> | <em>x</em><sub>0</sub>) = 
        &int; <em>p</em><sub>&theta;</sub>(<em>y</em><sub>0:t</sub>, <em>x</em><sub>1:t</sub> | <em>x</em><sub>0</sub>) 
        d<em>y</em><sub>1:t</sub> d<em>x</em><sub>1:t</sub>
    </p>
</div>
表示 𝑞(𝑦0|𝑥0) 的採樣分佈，其中 𝑝𝜃(𝑦𝑡|𝑥𝑡) 用於連接兩個域。

意涵：此公式表示從來源影像 𝑥0 到目標影像 𝑦0   的條件分佈（即翻譯過程的概率分佈）。  
分解：目標分佈 𝑝𝜃(𝑦0|𝑥0) 通過邊緣化中間變量（如前向過程的中間狀態 𝑥1:𝑡​ 和 𝑦1:𝑡 得到的。  
應用：該公式是整個翻譯模型的核心，描述如何從來源域到目標域進行分佈轉換。  

***variational lower bound***  
我們通過變分下界（Variational Lower Bound, VLB）優化負對數似然，提出以下引理：

Lemma 1： 負對數似然 −log⁡𝑝𝜃(𝑦0|𝑥0) 的上界為：
<div style="font-size: 20px; font-family: 'Times New Roman', serif; line-height: 1;">
    <p>&emsp;&emsp;&emsp;
        -log<em>p</em><sub>&theta;</sub>(<em>y</em><sub>0</sub> | <em>x</em><sub>0</sub>) ≤ 
        𝔼<sub><em>q</em></sub> 
        [
        log
        <em>p</em><sub>&theta;</sub>(<em>y</em><sub>0:t</sub>, <em>x</em><sub>1:t</sub> | <em>x</em><sub>0</sub>)
        /
        <em>q</em>(<em>y</em><sub>1:t</sub>, <em>x</em><sub>1:t</sub> | <em>y</em><sub>0</sub>, <em>x</em><sub>0</sub>)
        ]
    </p>
</div>
其中：
<div style="font-size: 20px; font-family: 'Times New Roman', serif; line-height: 1;">
    <p>&emsp;&emsp;&emsp;
        <em>q</em> = <em>q</em>(<em>y</em><sub>1:t</sub>, <em>x</em><sub>1:t</sub> | <em>y</em><sub>0</sub>, <em>x</em><sub>0</sub>)
    </p>
</div>

意涵：  
負對數似然（Negative Log-Likelihood, NLL）：我們希望最大化 𝑝𝜃(𝑦0|𝑥0) 的對數（或等價地最小化其負對數），以提高模型對目標分佈的擬合程度。  
變分下界：由於直接優化 −log⁡𝑝𝜃(𝑦0|𝑥0 通常很難計算，因此使用變分下界來近似目標。  
分子 𝑞(𝑦1:𝑡,𝑥1:𝑡|𝑦0,𝑥0)：真實的聯合分佈，用於描述真實數據的生成過程。  
分母 𝑝𝜃(𝑦0:𝑡,𝑥1:𝑡|𝑥0)：模型預測的聯合分佈，用於描述學習過程。  
應用：該公式提供了優化目標函數的方向，即讓模型分佈 𝑝𝜃 與真實分佈 𝑞 更接近。

***換句話說，翻譯映射可以通過優化以下變分下界 (VLB) 學習：***  
<div style="font-size: 20px; font-family: 'Times New Roman', serif; line-height: 1;">
    <p>&emsp;&emsp;&emsp;
        <em>L</em><sub>CE</sub> = 
        𝔼<sub>q(<em>y</em><sub>0</sub> | <em>x</em><sub>0</sub>)</sub> 
        [log<em>p</em><sub>&theta;</sub>(<em>y</em><sub>0</sub> | <em>x</em><sub>0</sub>)] 
        ≤ 
        𝔼<sub>q(<em>y</em><sub>0:t</sub>, <em>x</em><sub>1:t</sub> | <em>x</em><sub>0</sub>)</sub> 
        [
        log 
        <em>p</em><sub>&theta;</sub>(<em>y</em><sub>0:t</sub>, <em>x</em><sub>1:t</sub> | <em>x</em><sub>0</sub>) / 
        <em>q</em>(<em>y</em><sub>1:t</sub>, <em>x</em><sub>1:t</sub> | <em>y</em><sub>0</sub>, <em>x</em><sub>0</sub>)
        ]
        := <em>L</em><sub>VLB</sub>.
    </p>
</div>

意涵： https://imgur.com/a/PFbgffr


***高斯分佈假設：***  
我們首先聲明，最優解 𝑝𝜃(𝑦𝑡|𝑥𝑡) 是一個高斯分佈，並且其均值具有解析形式（見下述定理）。  
Theorem 1 （封閉形式表示）： 損失函數 𝐿𝐶𝐸 的封閉形式如下：
<div style="font-size: 20px; font-family: 'Times New Roman', serif; line-height: 1;">
    <p>&emsp;&emsp;&emsp;
        <em>L</em><sub>VLB</sub> = 
        &int; <em>E</em><sub>q(<em>y</em><sub>0</sub>,<em>x</em><sub>t</sub> | <em>x</em><sub>0</sub>)</sub>
        [<em>D</em><sub>KL</sub>(q(<em>y</em><sub>t</sub> | <em>y</em><sub>0</sub>) ∥ <em>p</em><sub>&theta;</sub>(<em>y</em><sub>t</sub> | <em>x</em><sub>t</sub>))] + <em>C</em>
    </p>
</div>
其中 𝐶 為非負常數。  

意涵： https://imgur.com/a/JciGCpI  

Theorem 2（最佳解）： 最優解 𝑝𝜃(𝑦𝑡|𝑥𝑡) 為高斯分佈，其均值為：
<div style="font-size: 20px; font-family: 'Times New Roman', serif; line-height: 1;">
    <p>&emsp;&emsp;&emsp;
        <em>μ</em><sub>&theta;</sub>(<em>x</em><sub>t</sub>) = 
        <em>α</em><sup>ˉ</sup><sub>t</sub> <em>y</em><sub>0</sub>
    </p>
</div>

意涵： https://imgur.com/a/WJtaaGH  
有關上述引理與定理的詳細證明，請參見線上附錄B。

### C. Reparameterization of μθ
***1. 擴散過程的均值表示***  
在目標域上訓練好的DDPM中，我們首先對來源數據 𝑥0 和目標數據 𝑦0 應用相同的前向擴散過程，作為共享編碼器，用以表示均值 𝜇𝜃(𝑥𝑡)：  
<div style="font-size: 20px; font-family: 'Times New Roman', serif; line-height: 1;">
    <p>&emsp;&emsp;&emsp;
        <em>x</em><sub>t</sub> = 
        <em>α</em><sup>ˉ</sup><sub>t</sub> <em>x</em><sub>0</sub> + 
        √(1 − <em>α</em><sup>ˉ</sup><sub>t</sub>) <em>z</em><sub>t</sub>, 
        &emsp;
        <em>y</em><sub>t</sub> = 
        <em>α</em><sup>ˉ</sup><sub>t</sub> <em>y</em><sub>0</sub> + 
        √(1 − <em>α</em><sup>ˉ</sup><sub>t</sub>) <em>z</em><sub>t</sub>.
    </p>
</div>

 * 公式解釋：
   * x𝑡 和 𝑦𝑡：分別為來源和目標數據在時間步 𝑡 的中間狀態。
   * αˉ𝑡​：累積變異數，控制擴散過程中的訊號強度。
   * z𝑡：標準高斯噪聲，加入擴散過程以擾亂數據。
 * 關鍵點：這一公式表明，在擴散過程中，來源和目標數據共享相同的噪聲分量 𝑧𝑡，僅受初始分佈 𝑥0 和 𝑦0 的影響。

***2. 參數化 𝜇𝜃(𝑥𝑡)***  
根據定理2，模型的目標是讓 𝜇𝜃(𝑥𝑡) 接近表達式 𝛼ˉ𝑡𝑦0，而 𝑥𝑡 是唯一的輸入數據。為此，我們引入以下參數化：
<div style="font-size: 20px; font-family: 'Times New Roman', serif; line-height: 1;">
    <p>&emsp;&emsp;&emsp;
        <em>μ</em><sub>θ</sub>(<em>x</em><sub>t</sub>) = 
        <em>f</em><sub>θ</sub>(<em>x</em><sub>t</sub>) − 
        √(1 − <em>α</em><sup>ˉ</sup><sub>t</sub>) 
        <em>z</em>(<em>x</em><sub>t</sub>).
    </p>
</div>

 * 符號說明：𝑓𝜃(𝑥𝑡)：可訓練函數，學習 𝑥𝑡 與 𝑦𝑡 的轉換映射。
𝑧(𝑥𝑡)：對應於 𝑧𝑡，是來源數據與目標數據的共享噪聲分量。
公式意義：該參數化將 𝜇𝜃(𝑥𝑡) 分為兩部分：
𝑓𝜃(𝑥𝑡)：對中間狀態的估計。
𝑧(𝑥𝑡)：消除噪聲的補償項。

***3. 損失函數***  
https://imgur.com/a/cQhKvYd

***4. 最終生成過程***  
https://imgur.com/a/25xVpma

這部分公式展示了如何使用共享的噪聲結構來參數化均值 𝜇𝜃(𝑥𝑡)，並通過最小化損失函數來學習生成模型的映射。這一設計有效利用了馬爾可夫過程的結構，實現高效的影像轉換任務。

### D. Determining an Appropriate Timestep for Translation
https://imgur.com/a/kCbkwuE




### E. Further Discussion of DMT
總整理  
https://imgur.com/a/lgNbmwF  
 * 多步方法：增加複雜度但不提升性能。
 * 非對稱方法：理論上可行，但最佳結果與對稱方法一致。
 * 實踐應用：簡化的DMT模組在性能和效率間取得良好平衡。



###  Fig. 4. 圖片說明
https://imgur.com/a/rOOYNIz



# IV. EXPERIMENTS
在本節中，我們針對四種不同的影像轉影像（I2I）任務評估所提出的 Diffusion Model Translator (DMT) 方法，包括影像風格化（image stylization）、影像上色（colorization）、分割影像生成（segmentation to image）以及草圖生成影像（sketch to image）。以下是各部分的重點：

***IV-B. DMT 域間轉換能力***  
我們首先展示 DMT 能夠在 I2I 任務中實現兩個域間的映射，驗證其在處理域轉換任務時的有效性。

***IV-C. 與代表性方法的比較***  
接著，我們將 DMT 與多種具代表性的現有方法進行比較，以證明其在效率與性能上的優勢。

***IV-D. 預設步驟 𝑡 的影響***  
最後，我們對影像翻譯中訓練時使用的預設步驟 𝑡 的影響進行消融研究（Ablation Study），深入探討步驟選擇對翻譯性能的作用。

### A. Experimental Setups
我們針對四個資料集進行 I2I 任務訓練，分別為我們自行手工製作的 Portrait 資料集（基於 CelebA-HQ 使用 QMUPD [42] 預訓練模型生成）、AFHQ [43]、CelebA-HQ [44] 以及 Edges2handbags [45], [46]。所有影像都調整為 256 × 256 的解析度。  
 * Portrait 資料集：包含 27,000 張用於訓練的影像和 3,000 張用於推論的影像，這些影像來自 CelebA-HQ 資料集。
 * AFHQ 資料集：包含 14,630 張訓練影像和 1,500 張推論影像，涵蓋貓、狗及野生動物的影像。
 * CelebA-HQ 資料集：隨機選取 27,000 張影像及其分割標籤作為配對訓練數據，其餘 3,000 張作為測試數據。
 * Edges2handbags 資料集：包含 138,567 張影像作為訓練數據及 200 張影像作為推論數據。

***評估指標***  
我們採用以下指標評估生成影像的質量與內容保持性：
1. Fréchet Inception Distance (FID) [40]
2. 結構相似性指數（SSIM） [41]
3. 感知影像片段相似性（LPIPS） [39]
4. L1 與 L2 誤差

此外，我們進行用戶研究，邀請用戶對影像質量從 1 到 5 進行打分。同時，對各方法的訓練與推論效率進行比較，包括總訓練週期數、每 1,000 張影像的訓練速度以及單張影像的推論時間。

***基線方法 ( Baselines )***  
我們將所提出的 DMT 方法與五種代表性的 I2I 演算法進行比較：
1. Pix2Pix [7]：一種經典的 cGAN 方法，採用 L1 和對抗損失。
2. TSIT [13]：基於 GAN 的多功能框架，使用特殊設計的正規化層及粗到細的特徵轉換。
3. SPADE [14]：專為語義影像合成設計的 GAN 框架，採用空間自適應正規化。
4. QMUPD [42]：針對人像風格化設計的 GAN 方法，進行無配對訓練；為公平比較，我們使用配對數據進行訓練。
5. Palette [9]：將 DDPM 框架引入 I2I 任務，並在每個降噪步驟中注入輸入約束。

這些方法可分為兩大類：GAN-based 和 DDPM-based。

***實現細節 ( Implementation Details )***  
我們使用 PyTorch [47] 平台在 Linux 環境下進行實驗，配備 NVIDIA Tesla A100 GPU。所有實驗的總時間步設置為 𝑇 = 1000，與 [2] 中的設置一致。
1. 逆向降噪過程使用帶有 Transformer 正弦嵌入的 U-Net 網絡骨幹 [48], [49]，並參考 [6] 的設置。
2. DDPM 在 DMT 模組訓練過程中保持凍結。
3. 為訓練 DMT 模組，我們使用 Pix2Pix [7] 和 TSIT [13] 模型，移除判別器模型僅訓練生成器部分，以確保翻譯器 𝑓𝜃​
  的功能形式近似於實際映射。

在推論過程中，DMT 使用 DDPM 的降噪過程，但這可能需要多次迭代計算，導致耗時。為提升效率，我們採用 DDIM [4]，實現僅需 10 次函數評估（NFE = 10）的高質量合成。

###  B. Qualitative Evaluation on Various Tasks
使用 DMT 推論影像的過程包含以下三個簡單步驟：
1. 前向擴散過程  
	將前向擴散過程應用於輸入影像 𝑥0，直到預先選定的時間步 𝑡，得到 𝑥𝑡，其公式為：  
	<div style="font-size: 20px; font-family: 'Times New Roman', serif; line-height: 1;">
		<p>&emsp;&emsp;&emsp;
			<em>x</em><sub>t</sub> = 
			<em>α</em><sub>t</sub><sup>ˉ</sup> 
			<em>x</em><sub>0</sub> + 
			√(1 - <em>α</em><sub>t</sub><sup>ˉ</sup>) 
			<em>z</em><sub>t</sub>
		</p>
	</div>
2. 估算中間結果 𝑦𝑡​  
	根據公式 (18)，使用功能逼近器 𝑓𝜃 獲得均值，並添加高斯噪聲推算近似結果 𝑦𝑡
3. 生成最終輸出  
	以 𝑦𝑡 作為中間結果，透過預訓練的 DDPM 的逆向過程進行採樣，生成所需的輸出影像。

***圖片與表格標註***  
 * 圖 5：DMT 與 SPADE [14] 在分割影像生成任務中的定性比較。DMT 展現出更高的影像質量與內容一致性。
 * 圖 6：DMT 與 QMUPD [42] 在影像風格化任務中的定性比較。DMT 生成的影像質量與內容一致性均優於 QMUPD。
 * 表 2：DMT 與 SPADE [14] 在分割影像生成任務中的定量比較。
 * 表 3：DMT 與 QMUPD [42] 在影像風格化任務中的定量比較。

***實驗結果與分析***  
我們在四個資料集上進行實驗，分別為 Portrait 資料集、AFHQ [43]、CelebA-HQ [44] 和 Edges2handbags [45], [46]。
 * 訓練設定：草圖生成影像任務使用 40 個訓練週期，其餘三個任務則使用 60 個週期。
 * 性能表現：如圖 2 所示，我們的方法能夠學習跨域翻譯映射，並生成高質量影像。例如，在風格化任務中，共享編碼器能夠區分兩個域的不同前向擴散過程。在其他任務中，即使輸入條件信息很少，我們的方法仍能提取輸入特徵，並生成具有高度多樣性且真實感強的影像。

### C. Comparisons
我們對所提出的方法與四種經典 I2I 方法進行定性和定量比較，這些方法包括 Pix2Pix [7]、TSIT [13]、SPADE [14]、QMUPD [42]，以及基於 DDPM 的條件生成方法 Palette [9]。

***與 SPADE [14] 的比較***  
 * 特性限制：SPADE 需要類別分割標籤（category-wise segmentation masks），因此應用範圍受限。相比之下，我們的方法透過對原始影像逐步添加噪聲，使用共享編碼器來破壞語義信息，避免了對分割標籤的依賴。
 * 任務範圍：由於此特性限制，我們僅在分割影像生成任務上與 SPADE 進行比較，並未直接在其基礎上應用 DMT。
 * 結果展示：結果如圖 5 和表 2 所示，DMT 展現出更優的影像質量與一致性。

***與 QMUPD [42] 的比較***  
 * 方法特點：QMUPD 專為無配對數據的人像生成設計，採用了質量指標指導的訓練過程。我們改用配對數據進行訓練，以提供更公平的比較基準。
 * 性能比較：結果如圖 6 和表 3 所示，我們的方法在質量與性能上達到甚至超越了現有標準。

***與 Palette [9] 的比較***  
 * 結果分析：如圖 8 所示，Palette 無法有效提取 CelebA-HQ 和 Edges2handbags 資料集中影像的分割特徵，導致生成的人像背景細節與手袋的馬匹圖案無法準確重現。
 * 方法優勢：相比之下，DMT 能夠生成高質量影像，即使在輸入語義信息較少的情況下，也能保留輸入條件的語義信息。

***與 Pix2Pix [7] 的比較***  
 * 觀察結果：DMT 生成的影像質量顯著高於 Pix2Pix。例如，Pix2Pix 在 CelebA-HQ 資料集中生成的人臉影像存在嚴重的瑕疵，而 DMT 始終能生成高質量的結果。
 * 特徵提取：共享編碼器與精心準備的 DDPM 模型顯著提升了我們方法的特徵提取能力。

***與 TSIT [13] 的比較***  
 * 特徵與質感：雖然 TSIT 引入了粗到細的特徵轉換塊，能在多數情況下生成高質量影像，但在輸入條件信息有限的情況下（如分割中的頭髮和額頭區域），其結果缺乏足夠的語義和紋理細節。
 * DMT 的表現：DMT 在額頭和頭髮區域生成了清晰的邊界和豐富的紋理。

**定量結果 ( The quantitative results )***  
 * 影像質量：表 4 顯示，我們的方法在影像保真度（FID）、感知損失（LPIPS）和結構相似性指數（SSIM）上表現最佳。
 * 訓練與推論效率：
   * DMT 所需的訓練週期數最少，生成 1,000 張影像的訓練速度最快，因為僅需訓練一個翻譯模組。
   * 與 Palette [9] 相比，DMT 的速度提高了 40 到 80 倍，其中包含採樣過程在中間步驟開始（提高 4 到 8 倍）以及使用快速採樣算法 DDIM（提高約 10 倍）。

### D. Ablation Study on the Timestep for Domain Translation
在 DMT 演算法中，我們透過共享解碼器逐步對 𝑥0 和 𝑦0 添加噪聲，直到某個預設時間步長 𝑡。時間步長 𝑡 對 translator 𝑓𝜃
 的性能以及生成影像的質量起著關鍵作用。  

***預設時間步長的選擇***  
如第 III-D 節所述，我們提出了一種簡單的方法，在訓練前選擇合適的時間步長 𝑡 = 𝑡*。該方法基於以下兩個距離的預計算：

1. (𝑥0,𝑥𝑡) 之間的距離。
2. (𝑥𝑡,𝑦𝑡) 之間的距離。

此方法有助於確定適合的 𝑡*，以在翻譯性能與生成質量間達到最佳平衡。

***時間步長 𝑡 的影響***  
圖 7 顯示了不同時間步長對生成結果的影響：
1. 步長較大時 (𝑡>400)：輸入條件對生成影像的限制減弱，導致結果出現不合理變化，例如，row 1 和 row 3 的臉部姿態明顯改變。
2. 步長較小時：翻譯映射難以逼近真實分布，例如，row 3, column 3 中影像的頭髮紋理失真。

***定量結果 ( present quantitative comparison results )***  
表 5 中展示了不同步長下生成影像的定量比較，顯示出輸入條件的限制強度與翻譯映射學習難度間的權衡。我們的方法選擇的步長 𝑡*  在性能上接近於表 5 中的最佳步長，證實了該簡單選擇策略的有效性。

***時間步長 𝑡 附近的研究***  
為進一步驗證該方法的強健性，我們對接近預設步長 𝑡* 的步長 𝑡 進行了消融研究。結果如表 6 和表 7 所示：
 * 即使在使用不同步長時性能有所下降，我們的策略仍能搜尋到適合 DMT 的步長。

### E. Limitations
我們的 DMT 方法存在一些限制，這些限制為未來研究提供了有趣的方向：  
1. Markovian property 的限制  
我們的演算法基於前向與逆向過程均滿足 Markovian property，但此假設僅適用於 DDPM 或其擴展模型。
2. 配對數據的依賴  
由於 DMT 依賴於 Pix2Pix [7] 或 TSIT [13] 模組作為翻譯映射 𝑓𝜃​
 ，因此僅設計用於配對數據的訓練，無法應用於無配對訓練數據及相關的影像轉影像任務。
3. 條件與目標域幾乎相同時的限制  
當條件（來源域）與目標域幾乎相同時，DMT 不適用。以下對此限制進行簡要說明：
根據公式 (13)，當 𝑥0 等於 𝑦0 時，𝑞(𝑦0|𝑥0) = 𝛿𝑥0(𝑦0)，這是狄拉克分佈（Dirac distribution）。因此，公式 (13) 變為：
<div style="font-size: 20px; font-family: 'Times New Roman', serif; line-height: 1;">
    <p>&emsp;&emsp;&emsp;
        <em>L</em><sub>CE</sub> = log <em>p</em><sub>θ</sub>(<em>x</em><sub>0</sub> ∣ <em>x</em><sub>0</sub>) = 0
    </p>
</div>
<p>&emsp;&emsp;這是一個與模型參數 𝜃 無關的常數，導致模型無法進行優化。


# V. CONCLUSION
在本研究中，我們提出了一種高效的擴散模型翻譯器（Diffusion Model Translator, DMT），用於連接經過充分訓練的 DDPM 與輸入推論過程。我們提供了理論證明，證明使用這一簡單模組來完成流行的影像轉影像（I2I）任務是可行的。

***主要貢獻***  
1. 實用方法：我們提出了一種實用方法，通過預先選擇合適的時間步長並應用前向擴散過程至該步長，將任務公式化為翻譯映射的學習過程，而無需對給定的 DDPM 進行任何再訓練。
2. 全面實驗：我們通過全面的實驗展示了所提出算法的高效性和卓越性能。

本研究成果不僅提升了 I2I 任務的性能，還為擴散模型在更多條件生成場景中的應用提供了新的可能性。








































































































































































































































































































































































